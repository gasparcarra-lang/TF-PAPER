# ============================ # LECAPs ‚Äî Pipeline Operativo (completo con correcciones) # ============================ # --- Imports --- import os import re import joblib import numpy as np import pandas as pd import random from hmmlearn.hmm import GaussianHMM from sklearn.preprocessing import StandardScaler from sklearn.experimental import enable_iterative_imputer # noqa: F401 from sklearn.impute import IterativeImputer from sklearn.calibration import CalibratedClassifierCV from sklearn.model_selection import KFold from sklearn.metrics import brier_score_loss, roc_auc_score from pandas.tseries.offsets import BDay from catboost import CatBoostClassifier from lightgbm import LGBMClassifier # === Anti-lookahead y features prohibidas === PROHIBIDAS = { 'OPEN_next', 'VWAP_next', 'CIERRE_plus5', 'ret_futuro_5d', 'ret_efectivo_open_5d', 'ret_efectivo_vwap_5d', 'ret_efectivo_open', 'ret_efectivo_vwap' } def _assert_no_future_tokens(cols): """ Detecta columnas con sufijos _next o _plus5 que indican fuga de datos (look-ahead). """ bad_tokens = ("_next", "_plus5") bad = [c for c in cols if any(t in c for t in bad_tokens)] if bad: raise ValueError(f"‚ùå Cols con lookahead detectadas: {bad}") def assert_no_forbidden_features(features): """ Lanza error si alguna de las features est√° en la lista PROHIBIDAS. """ bad = PROHIBIDAS.intersection(set(features)) if bad: raise ValueError(f"‚ùå Features prohibidas (futuras): {sorted(bad)}") np.random.seed(42); random.seed(42); os.environ["PYTHONHASHSEED"]="42" # --- Config general --- USE_VWAP_TO_CLOSE = True # True: VWAP_next ‚Üí CIERRE_plus5 | False: VWAP_next ‚Üí VWAP_plus5 top_n = 5 dias_entrenamiento = [2] # mi√©rcoles dia_recomendacion = 3 # jueves # === Rutas === path_lecaps = r"C:\Users\gaspar.carra\Desktop\LECAPs-Quant-Strategy\data\BASE LECAPS.xlsx" path_macro = r"C:\Users\gaspar.carra\Desktop\LECAPs-Quant-Strategy\data\SERIES TAMAR.REM.CCL16.5.xlsx" path_futuros = r"C:\Users\gaspar.carra\Desktop\LECAPs-Quant-Strategy\data\closing_prices_contratos .xlsx" path_info = r"C:\Users\gaspar.carra\Desktop\LECAPs-Quant-Strategy\data\LECAPS DATOS.xlsx" path_backtest = r"C:\Users\gaspar.carra\backtest_lecaps.csv" # Fecha de ‚Äúhoy‚Äù (normalizada) hoy = pd.Timestamp("2025-12-10").normalize() # hoy = pd.Timestamp.today().normalize() # --- Costos por s√≠mbolo (param√©tricos) --- COSTO_POR_SIMBOLO = { # "S29OCT25": {"comision_bps": 2.0, "spread_bps": 6.0, "impact_k": 20.0}, } DEFAULT_COST = {"comision_bps": 3.0, "spread_bps": 5.0, "impact_k": 20.0} def _get_cost_cfg(simbolo): return COSTO_POR_SIMBOLO.get(str(simbolo), DEFAULT_COST) # --- L√≠mites de liquidez --- ADV_TICKET_CAP = 0.05 # 5% ADV por ticket ADV_CARTERA_CAP = 0.20 # 20% ADV agregado MIN_DIAS_AL_VTO = 10 # ============ Utilitarios base ============ def parse_latam_number(series): s = series.astype(str).str.strip() if s.str.contains(",", regex=False).any(): s = s.str.replace(".", "", regex=False).str.replace(",", ".", regex=False) return pd.to_numeric(s, errors="coerce") def _coerce_numeric(df, cols): for c in cols: if c in df.columns: if df[c].dtype == "object": df[c] = parse_latam_number(df[c]) else: df[c] = pd.to_numeric(df[c], errors="coerce") return df def add_plus5_prices(df): df = df.sort_values(["SIMBOLO","FECHA"]).copy() df["CIERRE_plus5"] = df.groupby("SIMBOLO")["CIERRE"].shift(-5) df["VWAP_plus5"] = df.groupby("SIMBOLO")["VWAP"].shift(-5) return df def realized_return(row): buy = row.get("VWAP_next", np.nan) if pd.isna(buy) or buy <= 0: return np.nan sell = row.get("CIERRE_plus5", np.nan) if USE_VWAP_TO_CLOSE else row.get("VWAP_plus5", np.nan) return (sell / buy - 1) if pd.notna(sell) and sell > 0 else np.nan #============ Carga de datos ============ def load_data(path_lecaps, path_macro, path_futuros, vencimientos, sheet_lecaps=0, sheet_futuros='closing_prices_contratos'): # --- LECAPS --- df_lecaps = pd.read_excel(path_lecaps, sheet_name=sheet_lecaps) df_lecaps.columns = ( df_lecaps.columns.astype(str).str.strip().str.replace(r"\s+", "_", regex=True).str.upper() ) required_cols = ["SIMBOLO", "FECHA", "OPEN", "CIERRE", "VWAP", "MONTO_NEGOCIADO", "VALOR_NOMINAL"] missing = [c for c in required_cols if c not in df_lecaps.columns] if missing: raise ValueError(f"‚ùå Faltan columnas necesarias en df_lecaps: {missing}") df_lecaps["FECHA"] = pd.to_datetime(df_lecaps["FECHA"], errors="coerce").dt.normalize() df_lecaps["SIMBOLO"] = df_lecaps["SIMBOLO"].astype(str).str.strip().str.upper() df_lecaps = _coerce_numeric(df_lecaps, ["OPEN", "CIERRE", "VWAP", "MONTO_NEGOCIADO", "VALOR_NOMINAL"]) df_lecaps = df_lecaps.sort_values(["SIMBOLO", "FECHA"]).dropna(subset=["FECHA"]) # --- FUTUROS --- futuros_data = pd.read_excel(path_futuros, sheet_name=sheet_futuros) futuros_data.columns = ( futuros_data.columns.astype(str).str.strip().str.replace(r"\s+", "_", regex=True).str.upper() ) futuros_data['FECHA'] = pd.to_datetime(futuros_data['FECHA'], errors="coerce").dt.normalize() # --- BADLAR --- df_badlar = pd.read_excel(path_macro, sheet_name="BADLAR") df_badlar.columns = df_badlar.columns.astype(str).str.strip().str.upper() df_badlar = df_badlar.rename(columns={'VALOR': 'BADLAR'}) df_badlar['FECHA'] = pd.to_datetime(df_badlar['FECHA'], errors="coerce").dt.normalize() df_badlar['BADLAR'] = pd.to_numeric(df_badlar['BADLAR'], errors="coerce") # --- REM --- df_rem = pd.read_excel(path_macro, sheet_name="REM") df_rem.columns = df_rem.columns.astype(str).str.strip().str.upper() df_rem['FECHA'] = pd.to_datetime(df_rem['FECHA'], errors="coerce") df_rem = df_rem.drop_duplicates(subset='FECHA') df_rem = df_rem[df_rem['FECHA'] <= pd.to_datetime("2025-12-31")] if 'INFLACION_REM_DIARIA' in df_rem.columns: df_rem['INFLACION_REM_DIARIA'] = pd.to_numeric(df_rem['INFLACION_REM_DIARIA'], errors="coerce") if 'INFLACION_REM_ANUAL' in df_rem.columns: df_rem['INFLACION_REM_ANUAL'] = pd.to_numeric(df_rem['INFLACION_REM_ANUAL'], errors="coerce") df_rem = df_rem.set_index('FECHA').resample('D').ffill().reset_index() df_rem['FECHA'] = pd.to_datetime(df_rem['FECHA']).dt.normalize() # --- CCL --- df_ccl = pd.read_excel(path_macro, sheet_name="CCL") df_ccl.columns = df_ccl.columns.astype(str).str.strip().str.upper() df_ccl = df_ccl.rename(columns={'VALOR': 'SHOCK_CCL'}) df_ccl['FECHA'] = pd.to_datetime(df_ccl['FECHA'], errors="coerce").dt.normalize() df_ccl['SHOCK_CCL'] = pd.to_numeric(df_ccl['SHOCK_CCL'], errors="coerce") # --- RIESGO PAIS --- df_riesgo = pd.read_excel(path_macro, sheet_name="Riesgo_Pais") df_riesgo.columns = df_riesgo.columns.astype(str).str.strip().str.upper() df_riesgo = df_riesgo.rename(columns={'VALOR': 'RIESGO_PAIS'}) df_riesgo['FECHA'] = pd.to_datetime(df_riesgo['FECHA'], errors="coerce").dt.normalize() df_riesgo['RIESGO_PAIS'] = pd.to_numeric(df_riesgo['RIESGO_PAIS'], errors="coerce") # --- BRECHA CAMBIARIA --- df_brecha = pd.read_excel(path_macro, sheet_name="Brecha_Cambiaria") df_brecha.columns = df_brecha.columns.astype(str).str.strip().str.upper() df_brecha = df_brecha.rename(columns={'VALOR': 'BRECHA_CAMBIARIA'}) df_brecha['FECHA'] = pd.to_datetime(df_brecha['FECHA'], errors="coerce").dt.normalize() df_brecha['BRECHA_CAMBIARIA'] = pd.to_numeric(df_brecha['BRECHA_CAMBIARIA'], errors="coerce") # --- LICITACIONES --- df_lic = pd.read_excel(path_macro, sheet_name="Calendario de Licitaciones") df_lic.columns = df_lic.columns.astype(str).str.strip().str.upper() df_lic['FECHA'] = pd.to_datetime(df_lic['FECHA'], errors="coerce") fechas_licitaciones = ( df_lic.loc[df_lic['ES_LICITACION'].astype(str).str.upper() == 'SI', 'FECHA'] .dt.normalize().dropna().unique().tolist() ) fechas_canjes = [] return ( df_lecaps, futuros_data, df_badlar, df_rem, df_ccl, df_riesgo, df_brecha, fechas_licitaciones, fechas_canjes ) def verificar_hojas_excel_macro(path_macro, hojas_esperadas=None): if hojas_esperadas is None: hojas_esperadas = ["BADLAR", "REM", "CCL", "Riesgo_Pais", "Brecha_Cambiaria", "Calendario de Licitaciones"] errores = [] try: xl = pd.ExcelFile(path_macro) hojas_disponibles = xl.sheet_names except Exception as e: raise ValueError(f"‚ùå No se pudo abrir el archivo {path_macro}:\n{str(e)}") for hoja in hojas_esperadas: if hoja not in hojas_disponibles: errores.append(f"‚ùå Falta hoja: {hoja}") continue try: df = xl.parse(hoja) df.columns = df.columns.str.strip().str.upper() if hoja != "Calendario de Licitaciones": if "FECHA" not in df.columns: errores.append(f"‚ùå Falta columna 'FECHA' en hoja: {hoja}") if "VALOR" not in df.columns and hoja not in ["REM"]: errores.append(f"‚ùå Falta columna 'VALOR' en hoja: {hoja}") except Exception as e: errores.append(f"‚ùå Error al leer hoja {hoja}: {str(e)}") if errores: print("üîç Errores encontrados en el archivo macro:") for err in errores: print(" -", err) else: print("‚úÖ Todas las hojas necesarias est√°n presentes y bien formateadas.") return errores # ============ Feature engineering core ============ from scipy.interpolate import CubicSpline def clean_lecaps_data(df_lecaps, vencimientos, valores_finales, emisiones): vto_df = pd.DataFrame.from_dict(vencimientos, orient='index', columns=['VTO']).reset_index() vto_df.columns = ['SIMBOLO', 'VTO'] vto_df['VTO'] = pd.to_datetime(vto_df['VTO']) vf_df = pd.DataFrame.from_dict(valores_finales, orient='index', columns=['VALOR_FINAL']).reset_index() vf_df.columns = ['SIMBOLO', 'VALOR_FINAL'] emision_df = pd.DataFrame.from_dict(emisiones, orient='index', columns=['EMISION']).reset_index() emision_df.columns = ['SIMBOLO', 'EMISION'] emision_df['EMISION'] = pd.to_datetime(emision_df['EMISION']) df = df_lecaps.drop(columns=['VTO', 'VALOR_FINAL', 'EMISION'], errors='ignore') df = df.merge(vto_df, on='SIMBOLO', how='left') df = df.merge(vf_df, on='SIMBOLO', how='left') df = df.merge(emision_df, on='SIMBOLO', how='left') for col in ['VALOR_FINAL', 'CIERRE', 'OPEN', 'VWAP', 'MONTO_NEGOCIADO']: if col in df.columns: df[col] = pd.to_numeric(df[col], errors='coerce') df['DIAS_AL_VTO'] = (df['VTO'] - df['FECHA']).dt.days df['PLAZO_TOTAL'] = (df['VTO'] - df['EMISION']).dt.days df = df[df['DIAS_AL_VTO'] > 0].copy() df['DURACION'] = df['DIAS_AL_VTO'] / 365 df.loc[df['CIERRE'] <= 0, 'CIERRE'] = np.nan df['TIR'] = ((df['VALOR_FINAL'] / df['CIERRE']) - 1) * 365 / df['DIAS_AL_VTO'] df['carry'] = df['TIR'] df['Roll'] = df['carry'] * df['DURACION'] df['Curvatura'] = df.groupby('FECHA')['TIR'].transform( lambda x: x - x.rolling(3, center=True, min_periods=1).mean() ) return df def apply_spline_features(df): df = df.sort_values(['FECHA', 'DIAS_AL_VTO']).copy() df['Curvatura'] = df['TIR'] - df.groupby('FECHA')['TIR'].transform('mean') df['ModifiedDuration'] = df['DURACION'] df['Convexidad'] = df['DURACION'] ** 2 slope_values = [] for fecha, group in df.groupby('FECHA'): if group['DIAS_AL_VTO'].nunique() >= 4: try: x = group['DIAS_AL_VTO'].astype(float).values y = group['TIR'].astype(float).values spline = CubicSpline(x, y, bc_type='natural') slope = spline.derivative()(x) except Exception: slope = np.zeros(len(group)) else: slope = np.full(len(group), np.nan) slope_values.extend(slope) df['Slope'] = slope_values return df def process_futuros_data(futuros_data): fut = futuros_data.copy() fut.columns = ( fut.columns.astype(str).str.strip().str.replace(r"\s+", "_", regex=True).str.upper() ) cols = set(fut.columns) def pick(candidates, nombre_log): for c in candidates: if c in cols: return c raise KeyError(f"‚ùå No encontr√© columna para {nombre_log}. Prob√©: {candidates}. Disponibles: {sorted(list(cols))}") col_fecha = pick(["FECHA"], "FECHA") col_tipo = pick(["TIPO_CONTRATO", "TIPO_CONTRATO_", "CONTRATO", "COD_CONTRATO"], "TIPO_CONTRATO") col_rate = pick(["T._IMPLICITA","T_IMPLICITA","T. IMPLICITA","TASA_IMPLICITA","IMPLICITA","TASA_IMPL√çCITA"], "TASA_IMPLICITA") fut[col_fecha] = pd.to_datetime(fut[col_fecha], errors='coerce') rate = pd.to_numeric(fut[col_rate], errors='coerce') med = rate.median(skipna=True) if pd.notna(med) and med > 2: rate = rate / 100.0 fut[col_rate] = rate fut["VTO"] = pd.to_datetime( fut[col_tipo].astype(str).str.extract(r'(\d{6})')[0], format="%m%Y", errors="coerce" ) + pd.offsets.MonthEnd(0) fut["DIAS_AL_VTO"] = (fut["VTO"] - fut[col_fecha]).dt.days curva_fut = ( fut.groupby(col_fecha)[col_rate] .agg(["min","max","mean","std"]) .reset_index() .rename(columns={col_fecha:"FECHA","min":"tasa_min_fut","max":"tasa_max_fut","mean":"tasa_avg_fut","std":"vol_fut"}) ) curva_fut["Slope_Futuro"] = curva_fut["tasa_max_fut"] - curva_fut["tasa_min_fut"] curva_fut["FECHA"] = pd.to_datetime(curva_fut["FECHA"]).dt.normalize() return curva_fut def merge_futuros_lecaps(df_lecaps, curva_fut): curva_fut = curva_fut.drop_duplicates(subset=["FECHA"]) df_lecaps = df_lecaps.drop(columns=['tasa_min_fut','tasa_max_fut','tasa_avg_fut','vol_fut','Slope_Futuro'], errors='ignore') df_lecaps = df_lecaps.merge(curva_fut, on='FECHA', how='left') return df_lecaps def add_macro_and_events(df, df_badlar, df_rem, df_ccl, df_riesgo, df_brecha, fechas_licitaciones): df_badlar = df_badlar.drop_duplicates(subset=["FECHA"]) df_rem = df_rem.drop_duplicates(subset=["FECHA"]) df_ccl = df_ccl.drop_duplicates(subset=["FECHA"]) df_riesgo = df_riesgo.drop_duplicates(subset=["FECHA"]) df_brecha = df_brecha.drop_duplicates(subset=["FECHA"]) df = df.merge(df_badlar, on='FECHA', how='left') df = df.merge(df_rem, on='FECHA', how='left') df = df.merge(df_ccl, on='FECHA', how='left') df = df.merge(df_riesgo, on='FECHA', how='left') df = df.merge(df_brecha, on='FECHA', how='left') df['spread_TIR_Badlar'] = df['TIR'] - df['BADLAR'] df['spread_TIR_INF'] = df['TIR'] - df['INFLACION_REM_ANUAL'] fechas_validas = [d for d in fechas_licitaciones if d >= df['FECHA'].min()] if not fechas_validas: prox_vto = df['VTO'].min() fecha_estim_licitacion = prox_vto - pd.Timedelta(days=7) fechas_validas = [fecha_estim_licitacion] df['dias_a_licitacion'] = df['FECHA'].apply( lambda f: (min([d for d in fechas_validas if d >= f]) - f).days if any(d >= f for d in fechas_validas) else 60 ) df = df[df['FECHA'].dt.dayofweek < 5] return df def generate_targets_and_lags(df, entrenamiento=True): df = df.sort_values(['SIMBOLO', 'FECHA']).copy() df['TIR_lag1'] = df.groupby('SIMBOLO')['TIR'].shift(1) df['Roll_lag1'] = df.groupby('SIMBOLO')['Roll'].shift(1) df['Curvatura_lag1'] = df.groupby('SIMBOLO')['Curvatura'].shift(1) df['vol_tir_5d'] = df.groupby('SIMBOLO')['TIR'].transform(lambda x: x.rolling(5).std()) df['OPEN_next'] = df.groupby('SIMBOLO')['OPEN'].shift(-1) df['VWAP_next'] = df.groupby('SIMBOLO')['VWAP'].shift(-1) df = add_plus5_prices(df) if entrenamiento: df['ret_futuro_5d'] = df['CIERRE_plus5'] / df['CIERRE'] - 1 df['target_reg'] = df['ret_futuro_5d'].clip(lower=-0.2, upper=0.2) df = df[df['ret_futuro_5d'].between(-0.5, 0.5)] df['rank'] = df.groupby('FECHA')['ret_futuro_5d'].rank(method='first', ascending=False) df['target_rank'] = (df['rank'] <= 5).astype(int) df['ret_efectivo_vwap'] = df['VWAP_next'] / df['CIERRE'] - 1 df['ret_efectivo_open'] = df['OPEN_next'] / df['CIERRE'] - 1 df['ret_efectivo_vwap_5d'] = df['VWAP_plus5'] / df['VWAP_next'] - 1 df['ret_efectivo_open_5d'] = df['CIERRE_plus5'] / df['OPEN_next'] - 1 else: df['target_rank'] = np.nan; df['ret_futuro_5d'] = np.nan df['ret_efectivo_vwap_5d'] = df['VWAP_plus5'] / df['VWAP_next'] - 1 df['ret_efectivo_open_5d'] = df['CIERRE_plus5'] / df['OPEN_next'] - 1 df['ret_efectivo_vwap'] = df['VWAP_next'] / df['CIERRE'] - 1 df['ret_efectivo_open'] = df['OPEN_next'] / df['CIERRE'] - 1 return df.drop_duplicates(subset=["SIMBOLO", "FECHA"]) def build_model_dataset(df, n_regimenes=4): base_features = [ 'TIR','Roll','Curvatura','ModifiedDuration','Convexidad','Slope', 'vol_tir_5d','tasa_min_fut','tasa_max_fut','tasa_avg_fut','vol_fut', 'Slope_Futuro','TIR_lag1','Roll_lag1','Curvatura_lag1', 'BADLAR','INFLACION_REM_DIARIA','SHOCK_CCL', 'spread_TIR_Badlar','spread_TIR_INF','dias_a_licitacion', 'RIESGO_PAIS','BRECHA_CAMBIARIA','regimen', 'cambio_regimen','dias_en_regimen' ] for var in ['TIR','Roll','Curvatura','BADLAR','spread_TIR_Badlar']: base_features.extend([f"{var}_reg{r}" for r in range(n_regimenes)]) # columnas de "siguiente" quedan para diagn√≥stico; no se usar√°n como features del modelo base_features.append('regimen_siguiente_predicho') base_features.extend([f'prob_regimen_siguiente_{r}' for r in range(n_regimenes)]) features = [f for f in base_features if f in df.columns] return df.copy(), features # ============ HMM y r√©gimen ============ def entrenar_hmm(df, variables_hmm, n_regimenes=4): missing_cols = [col for col in variables_hmm if col not in df.columns] if missing_cols: raise ValueError(f"‚ùå Faltan variables para entrenar el HMM: {missing_cols}") df_hmm = df.dropna(subset=variables_hmm).copy() df_hmm["FECHA"] = pd.to_datetime(df_hmm["FECHA"]).dt.normalize() df_hmm = df_hmm.sort_values("FECHA") df_hmm[variables_hmm] = df_hmm[variables_hmm].astype(float) scaler = StandardScaler() X = scaler.fit_transform(df_hmm[variables_hmm]) modelo_hmm = GaussianHMM(n_components=n_regimenes, covariance_type="full", random_state=42, n_iter=500) modelo_hmm.fit(X) regimes = modelo_hmm.predict(X) df_resultado = df_hmm[["FECHA"]].copy() df_resultado["regimen"] = regimes return df_resultado, modelo_hmm, scaler def aplicar_hmm(df, modelo_hmm, scaler, variables_hmm, n_regimenes): missing_cols = [col for col in variables_hmm if col not in df.columns] if missing_cols: raise ValueError(f"‚ùå Faltan variables para aplicar HMM: {missing_cols}") df_ap = df.dropna(subset=variables_hmm).copy() df_ap["FECHA"] = pd.to_datetime(df_ap["FECHA"]).dt.normalize() df = df.copy(); df["FECHA"] = pd.to_datetime(df["FECHA"]).dt.normalize() df_ap = df_ap.sort_values("FECHA") df_ap[variables_hmm] = df_ap[variables_hmm].astype(float) X = scaler.transform(df_ap[variables_hmm]) regimes = modelo_hmm.predict(X) df_resultado = df_ap[["FECHA"]].copy() df_resultado["regimen"] = regimes df_resultado = df_resultado.drop_duplicates(subset="FECHA") df_merged = df.merge(df_resultado, on="FECHA", how="left") assert modelo_hmm.transmat_.shape[0] == n_regimenes, \ f"‚ùå El HMM entren√≥ con {modelo_hmm.transmat_.shape[0]} reg√≠menes, no {n_regimenes}" return df_merged def agregar_regimen_siguiente(df, modelo_hmm): transmat = modelo_hmm.transmat_ def regime_next(r): if r >= 0: return np.argmax(transmat[int(r)]) else: return -1 df = df.copy() df['regimen_siguiente_predicho'] = df['regimen'].apply(regime_next) for i in range(transmat.shape[0]): df[f'prob_regimen_siguiente_{i}'] = df['regimen'].apply( lambda r: transmat[int(r)][i] if r >= 0 else 0 ) return df# ============ Preparaci√≥n dataset ============ def preparar_dataset(df_lecaps, futuros_data, df_badlar, df_rem, df_ccl, df_riesgo, df_brecha, fechas_licitaciones, fechas_canjes, vencimientos, valores_finales, emisiones, variables_hmm=None, n_regimenes=4): df_lecaps = df_lecaps.drop_duplicates() df_lecaps = clean_lecaps_data(df_lecaps, vencimientos, valores_finales, emisiones) df_lecaps = apply_spline_features(df_lecaps) curva_fut = process_futuros_data(futuros_data) df_lecaps = merge_futuros_lecaps(df_lecaps, curva_fut) df_lecaps = add_macro_and_events( df_lecaps, df_badlar, df_rem, df_ccl, df_riesgo, df_brecha, fechas_licitaciones ) df_modelo = generate_targets_and_lags(df_lecaps.copy(), entrenamiento=True) df_lecaps_full = generate_targets_and_lags(df_lecaps.copy(), entrenamiento=False) if variables_hmm is None: variables_hmm = ['vol_tir_5d', 'spread_TIR_Badlar', 'BRECHA_CAMBIARIA', 'RIESGO_PAIS', 'SHOCK_CCL'] df_hmm_train = df_modelo[df_modelo["FECHA"] < df_modelo["FECHA"].max()] df_regimen, modelo_hmm, scaler_hmm = entrenar_hmm(df_hmm_train, variables_hmm, n_regimenes) df_modelo = aplicar_hmm(df_modelo, modelo_hmm, scaler_hmm, variables_hmm, n_regimenes) df_lecaps_full = aplicar_hmm(df_lecaps_full, modelo_hmm, scaler_hmm, variables_hmm, n_regimenes) df_modelo["regimen"] = df_modelo["regimen"].fillna(-1).astype(int) df_lecaps_full["regimen"] = df_lecaps_full["regimen"].fillna(-1).astype(int) df_modelo = df_modelo[df_modelo["regimen"] != -1].copy() df_lecaps_full = df_lecaps_full[df_lecaps_full["regimen"] != -1].copy() vars_interactuar = ['TIR','Roll','Curvatura','BADLAR','spread_TIR_Badlar'] for var in vars_interactuar: for r in range(n_regimenes): df_modelo[f"{var}_reg{r}"] = df_modelo[var] * (df_modelo["regimen"] == r).astype(int) df_lecaps_full[f"{var}_reg{r}"] = df_lecaps_full[var] * (df_lecaps_full["regimen"] == r).astype(int) df_modelo["cambio_regimen"] = (df_modelo["regimen"] != df_modelo["regimen"].shift(1)).astype(int) df_lecaps_full["cambio_regimen"] = (df_lecaps_full["regimen"] != df_lecaps_full["regimen"].shift(1)).astype(int) df_modelo["dias_en_regimen"] = df_modelo.groupby("SIMBOLO")["cambio_regimen"].cumsum() df_lecaps_full["dias_en_regimen"] = df_lecaps_full.groupby("SIMBOLO")["cambio_regimen"].cumsum() df_modelo = df_modelo[df_modelo["FECHA"] >= pd.to_datetime("2024-05-01")] # columnas de "siguiente" s√≥lo para diagn√≥stico; no se usan como features del modelo df_modelo = agregar_regimen_siguiente(df_modelo, modelo_hmm) df_lecaps_full = agregar_regimen_siguiente(df_lecaps_full, modelo_hmm) df_modelo, features = build_model_dataset(df_modelo, n_regimenes) df_lecaps_full, _ = build_model_dataset(df_lecaps_full, n_regimenes) df_lecaps_full["es_nuevo"] = df_lecaps_full[features].isnull().any(axis=1).astype(int) df_lecaps_full = df_lecaps_full.drop_duplicates(subset=["SIMBOLO", "FECHA"]) return df_lecaps_full, df_modelo, features # ============ Validaciones dataset ============ def assert_no_forbidden_features(features): bad = PROHIBIDAS.intersection(set(features)) if bad: raise ValueError(f"‚ùå Features prohibidas (futuras): {sorted(bad)}") def _filter_features_regimen(features): patt = re.compile(r"_reg\d+$") out = [] for f in features: if f in {"regimen","cambio_regimen","dias_en_regimen","regimen_siguiente_predicho"}: continue if f.startswith("prob_regimen_siguiente_"): continue if patt.search(f): continue out.append(f) return out def verificar_consistencia_dataset(df, features, variables_hmm=None, n_regimenes=None): errores = [] if 'FECHA' not in df.columns: errores.append("‚ùå Falta columna 'FECHA'.") elif not pd.api.types.is_datetime64_any_dtype(df['FECHA']): errores.append("‚ùå La columna 'FECHA' no est√° en formato datetime.") for col in features: if col not in df.columns: errores.append(f"‚ùå Falta feature requerida: {col}") elif df[col].dtype not in [np.float64, np.float32, np.int64, np.int32]: errores.append(f"‚ö†Ô∏è {col} no es num√©rica. Tipo: {df[col].dtype}") elif df[col].isnull().all(): errores.append(f"‚ö†Ô∏è {col} est√° completamente vac√≠a.") if variables_hmm: for col in variables_hmm: if col not in df.columns: errores.append(f"‚ùå Falta variable HMM: {col}") elif df[col].isnull().all(): errores.append(f"‚ö†Ô∏è Variable HMM sin datos: {col}") if 'target_rank' in df.columns and df['target_rank'].notna().sum() == 0: errores.append("‚ö†Ô∏è 'target_rank' no tiene valores v√°lidos.") if errores: print("üîç Errores encontrados en el dataset:") for err in errores: print(" -", err) else: print("‚úÖ Dataset consistente.") return errores # ============ Anti‚Äëleakage: CV purgada + embargo por d√≠as h√°biles ============ class PurgedKFoldEmbargo: """ Purged K-Fold con embargo por d√≠as h√°biles (BDay). Evita bleed-over temporal en panel: embarga H d√≠as h√°biles alrededor del fold de test. """ def __init__(self, n_splits=5, embargo_days=5): self.n_splits = n_splits self.embargo_days = int(embargo_days) def split(self, X, fechas: pd.Series): # X debe venir ordenado temporalmente fechas = pd.to_datetime(fechas).reset_index(drop=True) idx = np.arange(len(X)) # cortes por √≠ndice, embargo por fecha (BDay) folds = np.array_split(idx, self.n_splits) for k in range(self.n_splits): te = folds[k] te_min_date = fechas.iloc[te].min() te_max_date = fechas.iloc[te].max() left_border = te_min_date - BDay(self.embargo_days) right_border = te_max_date + BDay(self.embargo_days) mask_emb = fechas.between(left_border, right_border) tr = idx[~mask_emb.values] yield tr, te def _calibrar_modelo_cv_purgada(base_model, X, y, fechas, n_splits=5, embargo_days=5, method='sigmoid'): cv = list(PurgedKFoldEmbargo(n_splits=n_splits, embargo_days=embargo_days).split(X, fechas)) calibrado = CalibratedClassifierCV(estimator=base_model, cv=cv, method=method) calibrado.fit(X, y) return calibrado # ============ Entrenamiento rolling con HMM y REGRESI√ìN para retorno esperado ============ from catboost import CatBoostRegressor from lightgbm import LGBMRegressor def entrenar_modelos_regresion( df, features_base, ventana=30, n_regimenes=3, use_regimen=True, variables_hmm=('vol_tir_5d','spread_TIR_INF','SHOCK_CCL'), save_last_imputer=True ): df = df.sort_values("FECHA").copy() if "target_reg" not in df.columns: raise ValueError("‚ùå Falta 'target_reg' en df.") fechas = sorted(df["FECHA"].unique()) resultados = [] modelos_entrenados = {} last_feat_used = None assert_no_forbidden_features(features_base) for i in range(ventana, len(fechas) - 1): fecha_train = fechas[i - ventana:i] fecha_test = fechas[i] df_train = df[df["FECHA"].isin(fecha_train)].copy() df_test = df[df["FECHA"] == fecha_test].copy() # HMM SOLO con train hmm, scaler_hmm = _fit_hmm_on_window(df_train, list(variables_hmm), n_regimenes) use_regimen_window = (hmm is not None) and (scaler_hmm is not None) and use_regimen if use_regimen_window: df_train = _apply_hmm(df_train, hmm, scaler_hmm, list(variables_hmm)) df_test = _apply_hmm(df_test, hmm, scaler_hmm, list(variables_hmm)) _add_regime_interactions(df_train, n_regimenes, ['TIR','Roll','Curvatura','BADLAR','spread_TIR_Badlar']) _add_regime_interactions(df_test, n_regimenes, ['TIR','Roll','Curvatura','BADLAR','spread_TIR_Badlar']) else: df_train["regimen"] = -1 df_test["regimen"] = -1 feat_used = _features_for_window(list(features_base), n_regimenes, use_regimen_window) assert_no_forbidden_features(feat_used) X_train = df_train[feat_used].astype(float) y_train = df_train["target_reg"].astype(float).values X_test = df_test[feat_used].astype(float) if X_train.empty: continue imputer = IterativeImputer(random_state=42) imputer.fit(X_train) X_train = pd.DataFrame(imputer.transform(X_train), columns=feat_used, index=df_train.index) X_test = pd.DataFrame(imputer.transform(X_test), columns=feat_used, index=df_test.index) modelos = { "catboost": CatBoostRegressor(verbose=0, random_state=42), "lightgbm": LGBMRegressor(random_state=42) } preds = {} for nombre, model in modelos.items(): model.fit(X_train.values, y_train) modelos_entrenados[nombre] = model preds[nombre] = model.predict(X_test.values) ret_esperado = np.mean(list(preds.values()), axis=0) out = df_test.copy() out["ret_esperado"] = ret_esperado # columnas √∫tiles si faltan for c in ["FECHA","SIMBOLO","CIERRE","OPEN_next","VWAP_next","MONTO_NEGOCIADO", "ret_futuro_5d","ret_efectivo_open_5d","ret_efectivo_vwap_5d"]: if c not in out.columns: out[c] = np.nan resultados.append(out) # guardo artefactos de la √öLTIMA ventana (para inferencia jueves) if save_last_imputer: joblib.dump(imputer, "imputer_regresion.pkl") joblib.dump(feat_used, "features_used_regresion.pkl") last_feat_used = feat_used if not resultados: raise RuntimeError("‚ùå No se pudo generar backtest. Revis√° cobertura temporal/ventana.") df_backtest = pd.concat(resultados, ignore_index=True).drop_duplicates(["SIMBOLO","FECHA"]) df_backtest = df_backtest[df_backtest["MONTO_NEGOCIADO"] > 0] joblib.dump(modelos_entrenados, "modelos_regresion.pkl") if last_feat_used is not None: joblib.dump(last_feat_used, "features_used_regresion.pkl") with open("n_regimenes_backtest.txt","w") as f: f.write(str(n_regimenes)) print(f"‚úÖ Entrenamiento rolling de REGRESI√ìN listo. Fechas evaluadas: {df_backtest['FECHA'].nunique()}") print(f"üìà √öltimas features usadas (len={len(last_feat_used or [])}): {(last_feat_used or [])[:8]}...") return df_backtest # ============ M√©tricas para retorno esperado (regresi√≥n) ============ from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score def rolling_mse_by_bin(df, pred_col='ret_esperado', target_col='ret_futuro_5d', win=60, n_bins=10): """ Eval√∫a la estabilidad del modelo de retorno esperado usando rolling MSE por bins de predicci√≥n. """ out = [] fechas = sorted(df['FECHA'].unique()) for i in range(win, len(fechas)): f0, f1 = fechas[i - win], fechas[i] sl = df[(df['FECHA'] >= f0) & (df['FECHA'] <= f1)].dropna(subset=[pred_col, target_col]) if sl.empty: continue sl = sl.copy() sl['bin'] = pd.qcut(sl[pred_col], q=n_bins, duplicates='drop') stats = sl.groupby('bin').apply( lambda g: pd.Series({ 'MSE': mean_squared_error(g[target_col], g[pred_col]), 'MAE': mean_absolute_error(g[target_col], g[pred_col]), 'Bias': (g[pred_col] - g[target_col]).mean() }) ).reset_index().assign(window_end=f1) out.append(stats) return pd.concat(out, ignore_index=True) if out else pd.DataFrame() def r2_by_period_and_regime(df, pred_col='ret_esperado', target_col='ret_futuro_5d'): """ Eval√∫a R¬≤ por per√≠odo y r√©gimen. """ res = [] df = df.dropna(subset=[pred_col, target_col, 'regimen']) for r, g in df.groupby('regimen'): for y, gy in g.groupby(pd.Grouper(key='FECHA', freq='M')): if len(gy) < 5: continue r2 = r2_score(gy[target_col], gy[pred_col]) mae = mean_absolute_error(gy[target_col], gy[pred_col]) res.append({'regimen': int(r), 'periodo': y, 'R2': r2, 'MAE': mae}) return pd.DataFrame(res) def reliability_curve_regression(df, pred_col='ret_esperado', target_col='ret_futuro_5d', n_bins=10): """ Crea curva de confiabilidad para regresi√≥n: ¬øpromedio predicho ‚âà promedio observado? """ df = df.dropna(subset=[pred_col, target_col]).copy() df['bin'] = pd.qcut(df[pred_col], q=n_bins, duplicates='drop') grouped = df.groupby('bin').agg({ pred_col: 'mean', target_col: 'mean', 'FECHA': 'count' }).rename(columns={pred_col: 'pred_bin_mean', target_col: 'obs_bin_mean'}) return grouped.reset_index() ## ============ Costos y liquidez ============ def cost_model(row, comision_bps=None, spread_bps=None, adv_cap=0.05): cfg = _get_cost_cfg(row.get("SIMBOLO", "")) cbps = cfg["comision_bps"] if comision_bps is None else comision_bps sbps = cfg["spread_bps"] if spread_bps is None else spread_bps try: pct_adv = float(row.get("nominal_sugerido", 0.0)) / float(row["MONTO_NEGOCIADO"]) except Exception: pct_adv = 0.0 pct_adv = max(0.0, min(pct_adv, adv_cap)) impact_bps = cfg["impact_k"] * (pct_adv ** 0.7) total_bps = cbps + sbps + impact_bps return total_bps / 1e4 def slippage_only_bps(row, adv_cap=0.05): cfg = _get_cost_cfg(row.get("SIMBOLO","")) try: pct_adv = float(row.get("nominal_sugerido", 0.0)) / float(row["MONTO_NEGOCIADO"]) except Exception: pct_adv = 0.0 pct_adv = max(0.0, min(pct_adv, adv_cap)) return cfg["spread_bps"] + cfg["impact_k"] * (pct_adv ** 0.7) def _proxy_spread(row): v, c = row.get("VWAP", np.nan), row.get("CIERRE", np.nan) if pd.isna(v) or v <= 0 or pd.isna(c) or c <= 0: return np.nan return abs(v - c) / v def filter_liquidez(df_day): g = df_day.copy() g["proxy_spread"] = g.apply(_proxy_spread, axis=1) q1, q3 = g["proxy_spread"].quantile(0.25), g["proxy_spread"].quantile(0.75) iqr = q3 - q1 upper = q3 + 1.5 * iqr g = g[g["proxy_spread"] <= upper] g = g[g.get("DIAS_AL_VTO", 9999) >= MIN_DIAS_AL_VTO] return g def predecir_top_5_con_regimen_adaptativo( df, features, top_n=5, path_csv="backtest_lecaps_adaptativo.csv", df_backtest_hist=None, ventana=60, exposiciones_prev=None, capital_total=10_000_000, liquidity_cap=0.05, usar_vwap=False, dia_senal=3 # 3 = jueves ): modelos = joblib.load("modelos_calibrados.pkl") imputer = joblib.load("imputer.pkl") feat_used = joblib.load("features_used.pkl") assert_no_forbidden_features(feat_used) _assert_no_future_tokens(feat_used) hmm_last = joblib.load("hmm_last.pkl") if os.path.exists("hmm_last.pkl") else None sc_last = joblib.load("hmm_scaler_last.pkl") if os.path.exists("hmm_scaler_last.pkl") else None vars_hmm = joblib.load("hmm_variables.pkl") if os.path.exists("hmm_variables.pkl") else None n_reg_hmm = hmm_last.transmat_.shape[0] if hmm_last is not None else None df = df.copy() df['weekday'] = df['FECHA'].dt.dayofweek df = df[df['weekday'] == dia_senal].copy() fechas = sorted(df["FECHA"].unique()) resultados = [] recomendaciones_finales = [] for fecha in fechas: df_actual = df[df["FECHA"] == fecha].sort_values("FECHA").copy() df_actual = filter_liquidez(df_actual) if hmm_last is not None and sc_last is not None and vars_hmm is not None: df_actual = _apply_hmm(df_actual, hmm_last, sc_last, vars_hmm) _add_regime_interactions(df_actual, n_reg_hmm, ['TIR','Roll','Curvatura','BADLAR','spread_TIR_Badlar']) X = pd.DataFrame( imputer.transform(df_actual[feat_used].astype(float)), columns=feat_used, index=df_actual.index ) # ‚ö†Ô∏è Usamos predicci√≥n directa del retorno esperado (regresi√≥n) preds_ret = [m.predict(X) for m in modelos.values()] df_actual["ret_esperado"] = np.mean(preds_ret, axis=0) # Aplicar misma l√≥gica de selecci√≥n y caps mediana_monto = df_actual['MONTO_NEGOCIADO'].median() universo = df_actual[df_actual['MONTO_NEGOCIADO'] >= mediana_monto].copy() if universo.empty: universo = df_actual.copy() universo = universo.sort_values('ret_esperado', ascending=False).head(top_n).copy() suma = float(universo["ret_esperado"].sum()); n = int(len(universo)) if n == 0: continue universo["weight"] = (universo["ret_esperado"] / suma) if suma > 0 else (1.0 / n) universo["precio_compra"] = universo["CIERRE"] universo = universo.dropna(subset=["precio_compra", "MONTO_NEGOCIADO"]) universo = universo[(universo["precio_compra"] > 0) & (universo["MONTO_NEGOCIADO"] > 0)] def _retorno_neto_5d(row): ret_gross = float(row.get("ret_esperado", 0.0)) return ret_gross - 2.0 * cost_model(row) universo["ret_esperado_neto"] = universo.apply(_retorno_neto_5d, axis=1) universo["precio_venta_objetivo"] = universo["precio_compra"] * (1 + universo["ret_esperado_neto"]) universo["max_nominal_por_liquidez"] = universo["MONTO_NEGOCIADO"].astype(float) * ADV_TICKET_CAP universo["nominal_sugerido"] = capital_total * universo["weight"] universo["nominal_sugerido"] = np.minimum(universo["nominal_sugerido"], universo["max_nominal_por_liquidez"]) total_nominal = float(universo["nominal_sugerido"].sum()) max_cartera = float(df_actual["MONTO_NEGOCIADO"].sum()) * ADV_CARTERA_CAP if total_nominal > max_cartera and total_nominal > 0: universo["nominal_sugerido"] *= (max_cartera / total_nominal) universo["cantidad_titulos"] = universo["nominal_sugerido"] / universo["precio_compra"] resultados.append(df_actual.assign(weight=0.0)) recomendaciones_finales.append(universo[[ "FECHA","SIMBOLO","CIERRE","OPEN_next","VWAP_next","precio_compra", "precio_venta_objetivo","ret_esperado","ret_esperado_neto","weight", "MONTO_NEGOCIADO","nominal_sugerido","cantidad_titulos","DIAS_AL_VTO" ]].copy()) df_backtest = pd.concat(resultados, ignore_index=True).drop_duplicates(["SIMBOLO","FECHA"]) df_backtest.to_csv(path_csv, index=False) if recomendaciones_finales: df_recom = pd.concat(recomendaciones_finales, ignore_index=True) df_recom.to_csv("recomendaciones_operativas.csv", index=False) df_rec = pd.read_csv("recomendaciones_operativas.csv", parse_dates=['FECHA']) assert (df_rec['FECHA'].dt.dayofweek == dia_senal).all() else: df_recom = pd.DataFrame(columns=[ "FECHA","SIMBOLO","CIERRE","OPEN_next","VWAP_next","precio_compra", "precio_venta_objetivo","ret_esperado","ret_esperado_neto","weight", "MONTO_NEGOCIADO","nominal_sugerido","cantidad_titulos","DIAS_AL_VTO" ]) print(f"‚úÖ Backtest guardado en {path_csv}") print(f"üì§ Recomendaciones exportadas a recomendaciones_operativas.csv") return df_backtest, df_recom # ============ Lectura de info LECAPs ============ def get_info_lecaps(path_info): df_info = pd.read_excel(path_info) df_info.columns = ( df_info.columns.astype(str).str.strip().str.replace(r"\s+", "_", regex=True).str.upper() ) required_cols = ["SIMBOLO", "VTO", "VALOR_FINAL", "EMISION"] missing = [c for c in required_cols if c not in df_info.columns] if missing: raise ValueError(f"‚ùå Faltan columnas en {path_info}: {missing}") df_info["VTO"] = pd.to_datetime(df_info["VTO"], errors="coerce") df_info["EMISION"] = pd.to_datetime(df_info["EMISION"], errors="coerce") df_info["VALOR_FINAL"] = pd.to_numeric(df_info["VALOR_FINAL"], errors="coerce") vencimientos = df_info.set_index("SIMBOLO")["VTO"].to_dict() valores_finales = df_info.set_index("SIMBOLO")["VALOR_FINAL"].to_dict() emisiones = df_info.set_index("SIMBOLO")["EMISION"].to_dict() return vencimientos, valores_finales, emisiones # ===== Entrenamiento con SHAP estable orientado a retorno esperado (regresi√≥n) ===== from sklearn.impute import IterativeImputer from sklearn.inspection import permutation_importance from lightgbm import LGBMRegressor from catboost import CatBoostRegressor from sklearn.preprocessing import StandardScaler import joblib, numpy as np, pandas as pd from hmmlearn.hmm import GaussianHMM try: import shap _HAS_SHAP = True except Exception: _HAS_SHAP = False # Cross-validation con purga + embargo class PurgedKFoldEmbargo_SHAP: def __init__(self, n_splits=5, embargo_days=5): self.n_splits = n_splits self.embargo_days = embargo_days def split(self, X, fechas): fechas = pd.to_datetime(pd.Series(fechas)).dt.normalize().values idx = np.arange(len(X)) unique_dates = np.unique(fechas) folds = np.array_split(unique_dates, self.n_splits) for k in range(self.n_splits): test_dates = folds[k] tmin, tmax = test_dates.min(), test_dates.max() emb_lo = tmin - np.timedelta64(self.embargo_days, 'D') emb_hi = tmax + np.timedelta64(self.embargo_days, 'D') mask_train = (fechas < emb_lo) | (fechas > emb_hi) trn = idx[mask_train] tst = idx[np.isin(fechas, test_dates)] yield trn, tst # SHAP o Permutation Importances (adaptado para regresi√≥n) def _shap_stable_features(model_ctor, X, y, fechas, feature_names, n_splits=5, embargo_days=5, top_k=60, rank_agree=0.6): cv = list(PurgedKFoldEmbargo_SHAP(n_splits=n_splits, embargo_days=embargo_days).split(X, fechas)) picks = [] for trn, _ in cv: mdl = model_ctor() mdl.fit(X[trn], y[trn]) if _HAS_SHAP: explainer = shap.Explainer(mdl) sv = explainer(X[trn]) vals = np.mean(np.abs(sv.values), axis=0) imp = pd.Series(vals, index=feature_names).sort_values(ascending=False) else: imp_res = permutation_importance(mdl, X[trn], y[trn], n_repeats=10, random_state=42) imp = pd.Series(imp_res.importances_mean, index=feature_names).sort_values(ascending=False) picks.append(list(imp.head(min(top_k, len(feature_names))).index)) freq = pd.Series(0.0, index=feature_names) for p in picks: freq[p] += 1 freq /= len(picks) stable = list(freq[freq >= rank_agree].index) if len(stable) < max(20, len(feature_names)//3): stable = list(freq.sort_values(ascending=False).head(max(20, len(feature_names)//3)).index) return stable # HMM rolling + variables de r√©gimen def _fit_hmm_on_window(df_train, variables_hmm, n_regimenes): X = df_train[variables_hmm].astype(float).dropna() if X.empty: return None, None scaler = StandardScaler() Xs = scaler.fit_transform(X) hmm = GaussianHMM(n_components=n_regimenes, covariance_type="full", random_state=42, n_iter=500) hmm.fit(Xs) return hmm, scaler def _apply_hmm(df_part, hmm, scaler, variables_hmm): X = df_part[variables_hmm].astype(float) mask = X.notna().all(axis=1) regimes = pd.Series(np.nan, index=df_part.index) if mask.any(): Xs = scaler.transform(X.loc[mask]) regimes.loc[mask] = hmm.predict(Xs) out = df_part.copy() out["regimen"] = regimes.astype('float').fillna(-1).astype(int) return out def _add_regime_interactions(df, n_regimenes, vars_interactuar): for var in vars_interactuar: if var in df.columns: for r in range(n_regimenes): df[f"{var}_reg{r}"] = df[var] * (df["regimen"] == r).astype(int) return df def _features_for_window(base_features, n_regimenes, use_regimen): if use_regimen: feats = base_features + [ f"{v}_reg{r}" for v in ['TIR','Roll','Curvatura','BADLAR','spread_TIR_Badlar'] for r in range(n_regimenes) ] + ["regimen"] else: feats = base_features return list(dict.fromkeys(feats)) # sin duplicados # === Entrenamiento principal orientado a retorno esperado === def entrenar_modelos_regresion_shap_rolling( df, features_base, ventana=30, n_regimenes=3, use_regimen=True, variables_hmm=('vol_tir_5d','spread_TIR_INF','SHOCK_CCL') ): df = df.sort_values("FECHA").copy() if "target_reg" not in df.columns: raise ValueError("‚ùå Falta 'target_reg' (retorno futuro esperado).") fechas = sorted(df["FECHA"].unique()) resultados = [] modelos_regresion = {} last_feats = None last_hmm, last_scaler = None, None for i in range(ventana, len(fechas)-1): fecha_train = fechas[i-ventana:i] fecha_test = fechas[i] df_train = df[df["FECHA"].isin(fecha_train)].copy() df_test = df[df["FECHA"] == fecha_test].copy() hmm, scaler_hmm = _fit_hmm_on_window(df_train, list(variables_hmm), n_regimenes) use_reg = (hmm is not None) and (scaler_hmm is not None) and use_regimen if use_reg: df_train = _apply_hmm(df_train, hmm, scaler_hmm, list(variables_hmm)) df_test = _apply_hmm(df_test, hmm, scaler_hmm, list(variables_hmm)) _add_regime_interactions(df_train, n_regimenes, ['TIR','Roll','Curvatura','BADLAR','spread_TIR_Badlar']) _add_regime_interactions(df_test, n_regimenes, ['TIR','Roll','Curvatura','BADLAR','spread_TIR_Badlar']) last_hmm, last_scaler = hmm, scaler_hmm else: df_train["regimen"] = -1 df_test["regimen"] = -1 feat_used = _features_for_window(list(features_base), n_regimenes, use_reg) imp = IterativeImputer(random_state=42) Xtr = pd.DataFrame(imp.fit_transform(df_train[feat_used].astype(float)), columns=feat_used, index=df_train.index) Xte = pd.DataFrame(imp.transform(df_test[feat_used].astype(float)), columns=feat_used, index=df_test.index) ytr = df_train["target_reg"].astype(float).values def _model_ctor(): return LGBMRegressor(random_state=42, num_leaves=31, min_data_in_leaf=50, reg_lambda=1.0) stable_feats = _shap_stable_features( _model_ctor, Xtr.values, ytr, df_train["FECHA"].values, feature_names=feat_used, n_splits=3, embargo_days=3, top_k=min(60, len(feat_used)), rank_agree=0.65 ) XtrS, XteS = Xtr[stable_feats].values, Xte[stable_feats].values modelos = { "catboost": CatBoostRegressor(verbose=0, random_state=42, depth=6, l2_leaf_reg=5.0), "lightgbm": LGBMRegressor(random_state=42, num_leaves=31, min_data_in_leaf=50, reg_lambda=1.0) } preds = {} for nombre, model in modelos.items(): model.fit(XtrS, ytr) modelos_regresion[nombre] = (model, stable_feats, imp) preds[nombre] = model.predict(XteS) pred_ensamble = np.mean(list(preds.values()), axis=0) out = df_test.copy() out["ret_esperado"] = pred_ensamble resultados.append(out) last_feats = stable_feats if not resultados: raise RuntimeError("‚ùå No se gener√≥ backtest de regresi√≥n.") df_backtest = pd.concat(resultados, ignore_index=True).drop_duplicates(["SIMBOLO","FECHA"]) df_backtest = df_backtest[df_backtest["MONTO_NEGOCIADO"] > 0] class _WrapperReg: def __init__(self, model, feat): self.model = model self.feat = feat def predict(self, X_df): return self.model.predict(X_df[self.feat].values) modelos_wrapped = {k: _WrapperReg(v[0], v[1]) for k, v in modelos_regresion.items()} joblib.dump(modelos_wrapped, "modelos_regresion.pkl") joblib.dump(last_feats, "features_used_regresion.pkl") joblib.dump(imp, "imputer_regresion.pkl") if last_hmm and last_scaler: joblib.dump(last_hmm, "hmm_last.pkl") joblib.dump(last_scaler, "hmm_scaler_last.pkl") joblib.dump(list(variables_hmm), "hmm_variables.pkl") print(f"‚úÖ Regresi√≥n lista. Fechas: {df_backtest['FECHA'].nunique()} | Features estables: {len(last_feats)}") return df_backtest # ============ Main de ejecuci√≥n ============ USE_SHAP_CPCV_TRAINING = True # ‚Üê ponelo en True para usar la Celda 4A def estrategia_long_short(df_backtest, fecha, top_k=3, capital_total=10_000_000): df_day = df_backtest[df_backtest['FECHA'] == fecha].copy() df_day = df_day.dropna(subset=['ret_esperado', 'CIERRE', 'MONTO_NEGOCIADO']) if df_day.empty or len(df_day) < 2 * top_k: print(f"‚ö†Ô∏è No hay suficientes datos para {fecha}") return pd.DataFrame() df_day = df_day.sort_values('ret_esperado', ascending=False) longs = df_day.head(top_k).copy() shorts = df_day.tail(top_k).copy() longs['peso'] = longs['ret_esperado'] / longs['ret_esperado'].sum() shorts['peso'] = -shorts['ret_esperado'] / shorts['ret_esperado'].sum() longs['capital'] = longs['peso'] * (capital_total / 2) shorts['capital'] = shorts['peso'] * (capital_total / 2) for df in [longs, shorts]: df['nominal'] = df['capital'] df['cantidad_titulos'] = df['nominal'] / df['CIERRE'] df['side'] = np.where(df['peso'] > 0, 'LONG', 'SHORT') df_portfolio = pd.concat([longs, shorts], ignore_index=True) df_portfolio = df_portfolio[['FECHA', 'SIMBOLO', 'side', 'ret_esperado', 'peso', 'CIERRE', 'nominal', 'cantidad_titulos', 'MONTO_NEGOCIADO']] return df_portfolio.sort_values(by='ret_esperado', ascending=False) def evaluar_portafolio_ls(df_portafolio, df_backtest): df_eval = df_portafolio.merge( df_backtest[["FECHA", "SIMBOLO", "ret_futuro_5d"]], on=["FECHA", "SIMBOLO"], how="left" ) df_eval['pnl'] = df_eval['nominal'] * df_eval['ret_futuro_5d'] df_eval['ret_real'] = df_eval['ret_futuro_5d'] df_eval['pnl_pct'] = df_eval['pnl'] / df_eval['nominal'].abs() total_pnl = df_eval['pnl'].sum() total_capital = df_eval['nominal'].abs().sum() ret_total = total_pnl / total_capital if total_capital > 0 else np.nan print(f"üîç Evaluaci√≥n del portafolio L/S del {df_eval['FECHA'].iloc[0].date()}") print(f"üîπ PnL total: ${total_pnl:,.2f}") print(f"üîπ Retorno neto: {ret_total:.4%}") return df_eval.sort_values('side'), ret_total if __name__ == "__main__": n_reg_opt = 3 variables_opt = ['vol_tir_5d', 'spread_TIR_INF', 'SHOCK_CCL'] # 1) infos base vencimientos, valores_finales, emisiones = get_info_lecaps(path_info) # 2) sanity check del archivo macro errores_macro = verificar_hojas_excel_macro(path_macro) if errores_macro: raise ValueError("‚ùå Corrige los errores en el archivo macroecon√≥mico antes de continuar.") # 3) carga datasets df_lecaps, futuros_data, df_badlar, df_rem, df_ccl, df_riesgo, df_brecha, \ fechas_licitaciones, fechas_canjes = load_data( path_lecaps, path_macro, path_futuros, vencimientos=vencimientos ) # 4) preparar dataset (features + targets) df_lecaps_full, df_modelo, features = preparar_dataset( df_lecaps, futuros_data, df_badlar, df_rem, df_ccl, df_riesgo, df_brecha, fechas_licitaciones, fechas_canjes, vencimientos, valores_finales, emisiones, variables_hmm=variables_opt, n_regimenes=n_reg_opt ) # 5) definir features base (sin columnas de r√©gimen expandidas) features_base = _filter_features_regimen(features) # 6) validaciones errores = verificar_consistencia_dataset(df_modelo, features, variables_opt, n_reg_opt) if errores: raise ValueError("‚ùå Corrige los errores antes de continuar con el entrenamiento.") # 7) decidir si hay que regenerar hist√≥rico try: with open("n_regimenes_backtest.txt", "r") as f: n_reg_hist = int(f.read()) except FileNotFoundError: n_reg_hist = None reentrenar = ( not os.path.exists(path_backtest) or n_reg_hist is None or n_reg_hist != n_reg_opt ) # üîß Forzar reentrenamiento manualmente para debug reentrenar = True # 8) hist√≥rico (backtest) ‚Äî entrenar o cargar if reentrenar: print(f"‚ö†Ô∏è Generando hist√≥rico NUEVO con {n_reg_opt} reg√≠menes (regresi√≥n)...") df_backtest_hist = entrenar_modelos_regresion_shap_rolling( df=df_modelo, features_base=features_base, ventana=30, n_regimenes=n_reg_opt, use_regimen=True, variables_hmm=tuple(variables_opt) ) df_backtest_hist.to_csv(path_backtest, index=False) else: df_backtest_hist = pd.read_csv(path_backtest, parse_dates=['FECHA']) # 9) logging de d√≠a print(f"üìÖ Hoy es {hoy.strftime('%A %Y-%m-%d')}") # 10) d√≠a de entrenamiento semanal (p.ej. mi√©rcoles) if hoy.dayofweek in dias_entrenamiento: print("üß† Reentrenando modelos calibrados...") if USE_SHAP_CPCV_TRAINING: df_backtest_clasico = entrenar_modelos_calibrados_shap_cpcv( df=df_modelo, features_base=features_base, ventana=30, n_regimenes=n_reg_opt, use_regimen=True, variables_hmm=tuple(variables_opt), method_calib='isotonic' ) else: df_backtest_clasico = entrenar_modelos_calibrados( df_modelo, features_base, ventana=30, n_regimenes=n_reg_opt, use_regimen=True, variables_hmm=tuple(variables_opt) ) df_backtest_clasico.to_csv(path_backtest, index=False) # üéØ Ejecutar estrategia Long/Short con regresi√≥n fecha_rebalance = df_backtest_hist["FECHA"].max() df_portafolio_ls = estrategia_long_short(df_backtest_hist, fecha=fecha_rebalance, top_k=3) # üìä Evaluar performance real del portafolio df_eval, ret_total = evaluar_portafolio_ls(df_portafolio_ls, df_backtest_hist) # üíæ Guardar resultado df_eval.to_csv("estrategia_LS_" + fecha_rebalance.strftime("%Y%m%d") + ".csv", index=False) df_backtest_hist = df_backtest_clasico.copy() # 11) d√≠a de recomendaciones (p.ej. jueves) if hoy.dayofweek == dia_recomendacion: print("üìå Generando recomendaciones...") df_backtest_adapt, df_recom = predecir_top_5_con_regimen_adaptativo( df_lecaps_full, features_base, top_n=top_n, path_csv="backtest_lecaps_adaptativo.csv", df_backtest_hist=df_backtest_hist, ventana=60, usar_vwap=True # coherencia: compra VWAP_next, eval√∫a VWAP_plus5 (si USE_VWAP_TO_CLOSE=False) ) generar_reporte(df_recom.sort_values(["FECHA", "proba_promedio"], ascending=[True, False])) df_rec = pd.read_csv("recomendaciones_operativas.csv", parse_dates=['FECHA']) assert (df_rec['FECHA'].dt.dayofweek == dia_recomendacion).all() print("‚úÖ Recomendaciones generadas solo el d√≠a de se√±al")